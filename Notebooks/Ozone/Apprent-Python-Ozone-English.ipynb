{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"float:right; max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # [Statistical learning scenarios](https://github.com/wikistat/Apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Statistical Adaptation of an Ozone Peak Forecasting Model with <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> avec <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"Scikit-learn\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Abstract**: Exploration and modeling of climate data using Python and the [Scikit-learn library](http://scikit-learn.org/stable/#). The objective is to forecast for the following day a possible exceedance of an ozone concentration threshold from a deterministic forecast on a coarse mesh and local climatic variables. Estimation by different methods: [logistic regression](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [k nearest neighbors](http://wikistat.fr/pdf/st-m-app-add.pdf), [decision tree](http://wikistat.fr/pdf/st-m-app-cart.pdf), [model aggregation](http://wikistat.fr/pdf/st-m-app-agreg.pdf), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf). Comparison of [prediction errors](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf) on a test sample, then ROC curves. Iteration on several test samples to analyze forecast error distribution. This notebook complements the [study done with R](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Notebook-R-Ozone.html) to compare the two approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Warning** \n",
    "\n",
    "* This tutorial complements [the one in R](https://github.com/wikistat/MLTraining/tree/master/Notebooks/Ozone/Apprent-R-Ozone.ipynb) to compare the respective performances of the two environments: completeness of results and code efficiency. Explanations are more brief in this tutorial, which is usually run *after* or in parallel with the R tutorial.  \n",
    "* As with R, it is *divided into 5 tutorial sessions* *synchronized* with the machine learning course. \n",
    "* Not all options have been tested, and some are included as **exercises**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective, on these data, is to improve the deterministic forecast (MOCAGE), calculated by the services of MétéoFrance, of the ozone concentration in some sampling stations.  It is a problem of \"statistical adaptation\" of a local forecast of too large scale models with the help of other variables also managed by MétéoFrance, but on a smaller scale (temperature, wind strength...). This is a first way to design *IA hybrid* between a deterministic model and a machine learning algorithm. More precisely, two variables can be predicted: either the quantitative concentration of ozone, or the (qualitative) exceedance of a certain threshold set at $150$. In each case, two approaches are considered : either predict the *quantitative concentration* and then deduce the possible exceedance or directly predict the *exceedance*. In the first case, it is first a *regression* while in the second it is a two-class *discrimination* or logistic regression problem. \n",
    "\n",
    "The question is therefore: what are the best methods and strategies to predict the next day's ozone concentration on the one hand and the occurrence of a pollution peak on the other hand.\n",
    "\n",
    "We propose to test different methods: [logistic regression](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [discriminant analysis](http://wikistat.fr/pdf/st-m-app-add.pdf), [neural network](http://wikistat.fr/pdf/st-m-app-rn.pdf), [decision tree](http://wikistat.fr/pdf/st-m-app-cart.pdf), [tree aggregation](http://wikistat.fr/pdf/st-m-app-agreg.pdf) (random forest), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf).  The final objective is the comparison of these methods in order to determine the most efficient one to answer the forecasting problem. This requires the implementation of a very strict protocol to ensure a minimum of objectivity for this comparison.\n",
    "\n",
    "**Subsidiary question** When to choose R or Python? Python leads to results (conclusions) identical to those of R, less complete for their interpretation, but faster. These are the main differences between R for \"statisticians\" and Python for \"computer scientists\": you lose in interpretability but gain in execution speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 1 : Descriptive statistics and linear models </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data were extracted and formatted by the relevant department of Météo France. They are described by the following variables :\n",
    "\n",
    "* **JOUR**: type of day; holiday (1) or not (0) ;\n",
    "* **O3obs**: ozone concentration actually observed the next day at 5 pm local time, often corresponding to the maximum pollution observed;\n",
    "* **MOCAGE**: forecast of this pollution obtained by a deterministic fluid mechanics model (Navier and Stockes equation);\n",
    "* **TEMPE**: temperature forecast by Météo France for the next day at 5 pm;\n",
    "* **RMH2O** : humidity ratio;\n",
    "* **NO2** : nitrogen dioxide concentration;\n",
    "* **NO** : nitrogen monoxide concentration;\n",
    "* **STATION**: observation location: Aix-en-Provence, Rambouillet, Munchhausen, Cadarache and Plan de Cuques;\n",
    "* **WindMOD**: wind strength;\n",
    "* **WindANG**: wind direction. \n",
    "\n",
    "These are \"clean\" data, without missing values, well coded and small in size. They are therefore primarily of an educational nature, as they allow us to use and compare all the regression and supervised classification approaches.\n",
    "\n",
    "Here, we have chosen to read the data with the `pandas` library to benefit from the DataFrame class. This is not necessary for the forecasting objective, as the categorical variables thus constructed cannot be used to interpret the models obtained in `scikit-learn`, which does not recognize the DataFrame class.\n",
    "\n",
    "**Caution**: Even if the data are of good quality, a preliminary exploratory study is always necessary to become familiar with the data and prepare them for the modeling phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:10.756181Z",
     "start_time": "2019-11-18T09:19:10.033317Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path=\"\"\n",
    "ozone=pd.read_csv(path+\"depSeuil.dat\",sep=\",\",header=0)\n",
    "\n",
    "ozone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui suit permet d'affecter le bon type aux variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:10.784429Z",
     "start_time": "2019-11-18T09:19:10.762200Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone[\"STATION\"]=pd.Categorical(ozone[\"STATION\"],ordered=False)\n",
    "ozone[\"JOUR\"]=pd.Categorical(ozone[\"JOUR\"],ordered=False)\n",
    "ozone[\"O3obs\"]=pd.DataFrame(ozone[\"O3obs\"], dtype=float)\n",
    "ozone.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:10.823473Z",
     "start_time": "2019-11-18T09:19:10.787257Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the data have no particular flaws, a preliminary exploratory study is essential to ensure that they are consistent, to suggest possible transformations and to analyze correlation structures or, more generally, links between variables, groups of individuals or observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unidimensional statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:10.936092Z",
     "start_time": "2019-11-18T09:19:10.826112Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:11.125737Z",
     "start_time": "2019-11-18T09:19:10.937377Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ozone[\"O3obs\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:11.262897Z",
     "start_time": "2019-11-18T09:19:11.128038Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone[\"MOCAGE\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Traitez ainsi toutes les variables. Ceci suggère des transformations pour une meilleure utilisation des modèles linéaires.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:11.275823Z",
     "start_time": "2019-11-18T09:19:11.264575Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt, log\n",
    "ozone[\"SRMH2O\"]=ozone[\"RMH2O\"].map(lambda x: sqrt(x))\n",
    "ozone[\"LNO2\"]=ozone[\"NO2\"].map(lambda x: log(x))\n",
    "ozone[\"LNO\"]=ozone[\"NO\"].map(lambda x: log(x))\n",
    "del ozone[\"RMH2O\"]\n",
    "del ozone[\"NO2\"]\n",
    "del ozone[\"NO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Check the appropriateness of these transformations (histograms of the new variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the initial variables and construct the \"threshold exceeded\" variable below to obtain the file that will actually be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:11.297416Z",
     "start_time": "2019-11-18T09:19:11.279311Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone[\"DepSeuil\"]=ozone[\"O3obs\"].map(lambda x: x > 150)\n",
    "ozone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:16.123259Z",
     "start_time": "2019-11-18T09:19:11.299583Z"
    }
   },
   "outputs": [],
   "source": [
    "# scatter plot matrix for  quantitative variables\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(ozone[[\"O3obs\",\"MOCAGE\",\"TEMPE\",\"VentMOD\",\"VentANG\",\"SRMH2O\",\"LNO2\",\"LNO\"]], alpha=0.2, \n",
    "               figsize=(15, 15), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Comment on the relationships between the variables taken 2 by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:16.608748Z",
     "start_time": "2019-11-18T09:19:16.124724Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "# scaling of the  variables\n",
    "X=scale(ozone[[\"MOCAGE\",\"TEMPE\",\"VentMOD\",\"VentANG\",\"SRMH2O\",\"LNO2\",\"LNO\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classical numerical results are provided by the [PCA implementation](http://scikit-learn.org/stable/modules/decomposition.html) with `scikit-learn`, but more effort is needed to build the usual graphs usually automatically produced by dedicated libraries such as R's [FactoMineR](http://factominer.free.fr/) and [factoExtra](https://cran.r-project.org/web/packages/factoextra/index.html).\n",
    "\n",
    "The following commands allow you to perform a principal component analysis on quantitative variables only. In addition, the variable to be modeled (O3obs, observed concentration) is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:16.792776Z",
     "start_time": "2019-11-18T09:19:16.610421Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "## Estimation, computation of the principal components\n",
    "C = pca.fit(X).transform(X)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:17.039276Z",
     "start_time": "2019-11-18T09:19:16.798186Z"
    }
   },
   "outputs": [],
   "source": [
    "## distribution of the  principal components\n",
    "plt.boxplot(C[:,0:20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Comment on these results: which dimension was chosen?\n",
    "\n",
    "**Question** Presence of atypical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:19.969368Z",
     "start_time": "2019-11-18T09:19:17.040897Z"
    }
   },
   "outputs": [],
   "source": [
    "## Représentation of the individuals\n",
    "plt.figure(figsize=(5,5))\n",
    "for i, j, nom in zip(C[:,0], C[:,1], ozone[\"DepSeuil\"]):\n",
    "    color = \"red\" if nom  else \"blue\"\n",
    "    plt.plot(i, j, \"o\",color=color)\n",
    "plt.axis((-4,6,-4,6))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.137406Z",
     "start_time": "2019-11-18T09:19:19.973057Z"
    }
   },
   "outputs": [],
   "source": [
    "## coordinates and representation of the variables\n",
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j, nom in zip(coord1,coord2, ozone[[\"MOCAGE\",\"TEMPE\",\"VentMOD\",\n",
    "                                           \"VentANG\",\"SRMH2O\",\"LNO2\",\"LNO\"]].columns):\n",
    "    plt.text(i, j, nom)\n",
    "    plt.arrow(0,0,i,j,color='black')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='gray', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Comment on the correlation structure of the variables.\n",
    "\n",
    "**Question** The aim is to define a surface separating the two classes. Does a linear discriminaiton (hyperplane) seem possible?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The search for a better prediction method follows the following protocol.\n",
    "\n",
    "1. Preliminary uni- and multidimensional descriptive steps to identify inconsistencies, non-significant or exotic distribution variables, irrelevant or atypical individuals... and to study the data structures. It can also be the long step of building specific variables, attributes or *features* of the data. \n",
    "2. Randomly draw a *test* sample that will only be used in the *last step* of comparing methods.\n",
    "3. The remaining part is the *training* sample for estimating the parameters of the models.\n",
    "4. For each of the methods, optimize the complexity of the models by minimizing an \"unbiased\" estimate of the prediction error, e.g., by [*cross validation*](http://wikistat.fr/pdf/st-m-app-risque.pdf):\n",
    "    - Variables and interactions to be considered in linear or logistic regression;\n",
    "    - variables and method for discriminant analysis;\n",
    "    - number of leaves in the regression or classification tree;\n",
    "    - architecture (number of neurons, penalization) of the perceptron;\n",
    "    - aggregation algorithms, \n",
    "    - kernel and penalization of SVMs.\n",
    "5.  Comparison of predictive qualities based on the misclassification rate for the  test sample.\n",
    "\n",
    "\n",
    "\n",
    "**Notes**\n",
    "* In the case of a relatively \"small\" sample, it is recommended to iterate the learning/test splitting procedure ([cross-validation *Monte Carlo*](http://wikistat.fr/pdf/st-m-app-risque-estim.pdf)), in order to reduce the (average) variance of prediction error estimates.\n",
    "* *Caution*: do not \"cheat\" by modifying the model obtained in the previous step in order to improve the result on the test sample!\n",
    "* The criterion used depends on the problem: squared error, misclassification rate, AUC (area under the ROC curve), Pierce index, *log loss function*...\n",
    "* The \"choice\" of the best method can be replaced by a prediction combination, as is often the case with the \"winning\" but cumbersome soutions on the [kaggle site](https://www.kaggle.com/competitions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation for learning. \n",
    "\n",
    "**Question** Why are categorical variables transformed into indicator bundles or *dummy variables*?\n",
    "\n",
    "**Question** Why is the data frame type transformed into a matrix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.610161Z",
     "start_time": "2019-11-18T09:19:20.594438Z"
    }
   },
   "outputs": [],
   "source": [
    "ozone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.645052Z",
     "start_time": "2019-11-18T09:19:20.611990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Explanatory Variables\n",
    "ozoneDum=pd.get_dummies(ozone[[\"JOUR\",\"STATION\"]])\n",
    "del ozoneDum[\"JOUR_0\"]\n",
    "ozoneQuant=ozone[[\"MOCAGE\",\"TEMPE\",\"VentMOD\",\"VentANG\",\"SRMH2O\",\"LNO2\",\"LNO\"]]\n",
    "dfC=pd.concat([ozoneDum,ozoneQuant],axis=1)\n",
    "dfC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.652330Z",
     "start_time": "2019-11-18T09:19:20.647878Z"
    }
   },
   "outputs": [],
   "source": [
    "# binary variable  to predict\n",
    "Yb=ozone[\"DepSeuil\"].map(lambda x: int(x))\n",
    "# Real variable to predict\n",
    "Yr=ozone[\"O3obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.824319Z",
     "start_time": "2019-11-18T09:19:20.653924Z"
    }
   },
   "outputs": [],
   "source": [
    "Yr.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of training and test samples for both types of model. As the generator is initialized identically, the same samples are used in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.838969Z",
     "start_time": "2019-11-18T09:19:20.825953Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train,X_test,Yb_train,Yb_test=train_test_split(dfC,Yb,test_size=200,random_state=11)\n",
    "X_train,X_test,Yr_train,Yr_test=train_test_split(dfC,Yr,test_size=200,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is data standardization or normalization. The variables are divided by their standard deviation. This is not useful in the case of an elementary linear model, as the solution is identical, but it is essential for many other non-linear methods (SVMs, neural networks, penalized models). This step is therefore systematically carried out to avoid any problems. \n",
    "\n",
    "**Please note**: the same parameters (means, standard deviations) estimated on the training sample are used to normalize the test sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:20.849578Z",
     "start_time": "2019-11-18T09:19:20.840871Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "# The neural network algorithm may require normalization of the \n",
    "# of the explanatory variables with the commands below\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xr_train = scaler.transform(X_train)  \n",
    "# Same transformation for the test\n",
    "Xr_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction by linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear and generalized linear model functions are limited in [Scikit-learn](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) and without detailed numerical (test) outputs. It is preferable to use another library [StatsModels](http://statsmodels.sourceforge.net/stable/examples/notebooks/generated/glm.html) whose outputs are inspired by those of R. In both cases, the classical strategies (forward, backward, stepwise) of variable selection by optimization of a criterion (Cp, AIC, BIC) do not seem to be available, even if AIC and BIC are present in `scikit-learn`, and the DataFrame type (package `pandas`) is not recognized.\n",
    "\n",
    "The most efficient way to proceed is therefore to introduce a [Lasso penalization](http://wikistat.fr/pdf/st-m-app-select.pdf) to carry out variable selection, or rather the selection of quantitative variables and indicators of the modalities of qualitative ones, but without detailed analysis of interactions as is possible with R.\n",
    "\n",
    "**Question** What other type of penalization is also used in regression?\n",
    "\n",
    "**Question** Which method combines the two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary comparison  with  MOCAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we plot the concentration prediction for the test sample using only the value of the *Mocage* model, as well as the residuals of this model as a function of the predicted value (Mocage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.112432Z",
     "start_time": "2019-11-18T09:19:20.851395Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_train[\"MOCAGE\"],Yr_train,\"o\")\n",
    "plt.xlabel(\"Mocage\")\n",
    "plt.ylabel(\"O3 observee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.118987Z",
     "start_time": "2019-11-18T09:19:21.114624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(Yr_train,X_train[\"MOCAGE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.259504Z",
     "start_time": "2019-11-18T09:19:21.121039Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_test[\"MOCAGE\"],Yr_test,\"o\")\n",
    "plt.xlabel(\"Mocage\")\n",
    "plt.ylabel(\"O3 observee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.436658Z",
     "start_time": "2019-11-18T09:19:21.261005Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_test[\"MOCAGE\"],X_test[\"MOCAGE\"]-Yr_test,\"o\")\n",
    "plt.xlabel(\"Mocage\")\n",
    "plt.ylabel(\"Residus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Comment on the quality of these residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.445464Z",
     "start_time": "2019-11-18T09:19:21.439951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(X_test[\"MOCAGE\"],Yr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.454376Z",
     "start_time": "2019-11-18T09:19:21.447037Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(Yr_test,X_test[\"MOCAGE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this deterministic forecast (Navier and Stockes equation) with the most elementary statistical adaptation. This is a regression with model choice by regularization with a Lasso penalty. \n",
    "\n",
    "**Question** What is the default value of the Lasso penalty parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.465143Z",
     "start_time": "2019-11-18T09:19:21.457333Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regLasso = linear_model.Lasso()\n",
    "regLasso.fit(Xr_train,Yr_train)\n",
    "prev=regLasso.predict(Xr_test)\n",
    "print(\"MSE=\",mean_squared_error(Yr_test,prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:21.471807Z",
     "start_time": "2019-11-18T09:19:21.466586Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(Yr_test,prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso penalty parameter is optimized by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.608300Z",
     "start_time": "2019-11-18T09:19:21.473424Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Grid of values for the parameter  alpha to optimize\n",
    "param=[{\"alpha\":[0.05,0.1,0.2,0.3,0.4,0.5,1]}]\n",
    "regLasso = GridSearchCV(linear_model.Lasso(), param,cv=5,n_jobs=-1)\n",
    "regLassOpt=regLasso.fit(Xr_train, Yr_train)\n",
    "# optimal parameter\n",
    "regLassOpt.best_params_[\"alpha\"]\n",
    "print(\"Meilleur R2 = %f, Meilleur paramètre = %s\" % (regLassOpt.best_score_,regLassOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which cross-validation is performed?\n",
    "\n",
    "Obtain a forecast with the optimal value of `alpha` then calculate and plot residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.616625Z",
     "start_time": "2019-11-18T09:19:23.610359Z"
    }
   },
   "outputs": [],
   "source": [
    "prev=regLassOpt.predict(Xr_test)\n",
    "print(\"MSE=\",mean_squared_error(prev,Yr_test))\n",
    "print(\"R2=\",r2_score(Yr_test,prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.773914Z",
     "start_time": "2019-11-18T09:19:23.618387Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(prev,Yr_test,\"o\")\n",
    "plt.xlabel(u\"O3 Prédite\")\n",
    "plt.ylabel(\"O3 observee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.955256Z",
     "start_time": "2019-11-18T09:19:23.778464Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(prev,Yr_test-prev,\"o\")\n",
    "plt.xlabel(u\"Prédites\")\n",
    "plt.ylabel(u\"Résidus\")\n",
    "plt.hlines(0,40,220)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare these residuals with the previous ones (mocage) and note the improvement. \n",
    "\n",
    "**Question** Comment on the shape of the cloud and therefore the validity of the model. \n",
    "\n",
    "Interpretation requires knowing the values of the model's coefficients, whereas the `regLassOpt` object from `GridSearchCV` does not retain the estimated parameters. It must therefore be re-estimated with the optimal value of the penalty parameter if we wish to display these coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.966794Z",
     "start_time": "2019-11-18T09:19:23.959069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regLasso=linear_model.Lasso(alpha=regLassOpt.best_params_['alpha'])\n",
    "model_lasso=regLasso.fit(Xr_train,Yr_train)\n",
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:23.973385Z",
     "start_time": "2019-11-18T09:19:23.968424Z"
    }
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
    "print(\"Lasso conserve \" + str(sum(coef != 0)) + \n",
    "      \" variables et en supprime \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:24.238690Z",
     "start_time": "2019-11-18T09:19:23.974918Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(u\"Coefficients du modèle lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Note the consequences of penalization; interpret the effect of each variable on ozone concentration.\n",
    "\n",
    "This is where the python library comes up short. You'd have to build \"by hand\" or use the `Statsmodels` library to display test statistics and p-values. Even with these additions, there is no provision for taking interactions and their selection into account. Furthermore, interpretation is complicated by the fact that each qualitative variable is broken down into packets of indicators. This is still understandable with a small number of variables, but quickly becomes unworkable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph shows the good and bad forecasts for exceeding the legal threshold, here set at $ 150 \\mu g $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:24.418626Z",
     "start_time": "2019-11-18T09:19:24.240884Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(prev,Yr_test,\"o\")\n",
    "plt.xlabel(u\"Valeurs prédites\")\n",
    "plt.ylabel(u\"O3 observée\")\n",
    "plt.hlines(150,50,300)\n",
    "plt.vlines(150,0,300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:24.443895Z",
     "start_time": "2019-11-18T09:19:24.420117Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "table=pd.crosstab(prev>150,Yr_test>150)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Observe the asymmetry of this matrix. What is it at least partly due to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lassoCV` uses a *coordinate descent* algorithm, with no derivative calculation since the *l1* norm is not derivable, while `lassoLarsCV` is based on the *least angle regression* algorithm. These functions also plot *regularization paths*. Here's an example of `lassoCV`, which offers more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:24.818339Z",
     "start_time": "2019-11-18T09:19:24.446157Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "model = LassoCV(cv=5, alphas=np.array(range(1,50,1))/20.,n_jobs=-1,random_state=13).fit(Xr_train,Yr_train)\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "# ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='MSE moyen', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: optimal par VC')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE for each validation: coordinate descent ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Check that this is the same optimal value as the one you found previously.\n",
    "\n",
    "Drawing of regularization paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.069976Z",
     "start_time": "2019-11-18T09:19:24.819818Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "from sklearn.linear_model import lasso_path\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(Xr_train,Yr_train, alphas=np.array(range(1,50,1))/20.,)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "for coef_l, s in zip(coefs_lasso, styles):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, linestyle=s,c='b')\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression or binomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach is used, but directly modeling the binary variable of whether or not the threshold is exceeded. This is a logistic regression, with a Lasso penalty to select the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.074229Z",
     "start_time": "2019-11-18T09:19:25.071680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.292166Z",
     "start_time": "2019-11-18T09:19:25.080838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimisation of the penalisation parameter\n",
    "# grid of values\n",
    "param=[{\"C\":[1,1.2,1.5,1.7,2,3,4]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver=\"liblinear\"), param,cv=5,n_jobs=-1)\n",
    "logitOpt=logit.fit(Xr_train, Yb_train) \n",
    "# optimal parameter\n",
    "logitOpt.best_params_[\"C\"]\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1.-logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.301641Z",
     "start_time": "2019-11-18T09:19:25.295698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Error on the test sample\n",
    "1-logitOpt.score(Xr_test, Yb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"optimal\" model obtained is used to predict the test sample and thus estimate, without bias, a prediction error. \n",
    "\n",
    "The confusion matrix cross-references predicted threshold violations with those actually observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.324130Z",
     "start_time": "2019-11-18T09:19:25.303337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "y_chap = logitOpt.predict(Xr_test)\n",
    "# confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the model is based on the values of the coefficients, with the same difficulties or restrictions as for regression. Please note that `GridSearch` does not retain the coefficients, which must be re-estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.342364Z",
     "start_time": "2019-11-18T09:19:25.326204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "logitLasso=LogisticRegression(penalty=\"l1\",C=logitOpt.best_params_['C'],\n",
    "                              solver=\"liblinear\")\n",
    "logitCoef=logitLasso.fit(Xr_train,Yb_train).coef_\n",
    "print(logitCoef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.352838Z",
     "start_time": "2019-11-18T09:19:25.345009Z"
    }
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(logitCoef[0], index = X_train.columns)\n",
    "print(\"Lasso conserve \" + str(sum(coef != 0)) + \n",
    "      \" variables et en supprime \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.597658Z",
     "start_time": "2019-11-18T09:19:25.354622Z"
    }
   },
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(u\"Coefficients du modèle lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Interpret the effect of the variables selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:25.803353Z",
     "start_time": "2019-11-18T09:19:25.599427Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "probas_ = LogisticRegression(penalty=\"l1\", solver=\"liblinear\",\n",
    "                    C=logitOpt.best_params_['C']).fit(X_train, Yb_train).predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(Yb_test, probas_[:,1])\n",
    "plt.plot(fpr, tpr, lw=1)\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Comment on the ROC curve regarding the choice of threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 2 :  KNN, SVM </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a case study of [non-parametric discriminant analyses](http://scikit-learn.org/stable/modules/neighbors.html). Linear and quadratic [parametric discriminant analyses](http://scikit-learn.org/stable/modules/lda_qda.html) (Gaussian) are also available in `scikit-learn`, but are left as an exercise.\n",
    "\n",
    "The complexity parameter $K$ is optimized on a predefined grid, minimizing the estimated error by cross-validation; `scikit-learn` offers a number of cross-validation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.282117Z",
     "start_time": "2019-11-18T09:19:25.805488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Optimisation of k\n",
    "# grid of values\n",
    "param_grid=[{\"n_neighbors\":list(range(1,15))}]\n",
    "knn=GridSearchCV(KNeighborsClassifier(),param_grid,cv=5,n_jobs=-1)\n",
    "knnOpt=knn.fit(Xr_train, Yb_train)  \n",
    "# optimal parameter\n",
    "knnOpt.best_params_[\"n_neighbors\"]\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1.-knnOpt.best_score_,knnOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.301358Z",
     "start_time": "2019-11-18T09:19:26.284307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimation  of the prediction error on the test sample\n",
    "1-knnOpt.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.352100Z",
     "start_time": "2019-11-18T09:19:26.304330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction on the test sample\n",
    "y_chap = knnOpt.predict(Xr_test)\n",
    "# confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Complete the results using the function [KNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) to model the concentration; optimize $K$, calculate the test sample prediction, plot the residuals, calculate the MSE on the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [*Support Vector Machine*](http://wikistat.fr/pdf/st-m-app-svm.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many parameters are associated with this method. The list can be consulted in the online [documentation](http://scikit-learn.org/stable/modules/svm.html).\n",
    "\n",
    "Penalization optimization (parameter C) is sought on a grid by cross-validation.\n",
    "\n",
    "Note: it would also be necessary to optimize the value of the *gamma* coefficient linked to the Gaussian kernel (\"standard deviation\").\n",
    "\n",
    "It is often necessary to normalize data before operating SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "param=[{\"C\":[0.4,0.5,0.6,0.8,1,1.4]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(Xr_train, Yb_train)\n",
    "# optimal parameter\n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on the test sample\n",
    "1-svmOpt.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the  test sample\n",
    "y_chap = svmOpt.predict(Xr_test)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** As before, replace the SVC function with the [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) regression function. Optimize the parameter, calculate the forecast, the residuals and the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 3 :  CART, Random Forests  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### [Binary decision trees (CART)](http://wikistat.fr/pdf/st-m-app-cart.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Binary classification  and regression trees](http://scikit-learn.org/stable/modules/tree.html) are well implemented in scikit-learn, but their pruning is inadequate. It's not complexity that is penalized, and therefore precisely the number of leaves that is optimized, but the overall depth of the tree, at the risk of pruning, at a given depth, important leaves or keeping ambiguous leaves.\n",
    "\n",
    "As before, cross-validation is used to optimize the parameter on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.625493Z",
     "start_time": "2019-11-18T09:19:26.354381Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Optimisation of the tree depth\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "tree= GridSearchCV(DecisionTreeClassifier(),param,cv=10,n_jobs=-1)\n",
    "treeOpt=tree.fit(Xr_train, Yb_train)\n",
    "# optimal paramter\n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - treeOpt.best_score_,treeOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.632813Z",
     "start_time": "2019-11-18T09:19:26.627275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimation of the prediction error\n",
    "1-treeOpt.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.664280Z",
     "start_time": "2019-11-18T09:19:26.634973Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_chap = treeOpt.predict(Xr_test)\n",
    "# confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the obtained tree with  `plot_tree()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.806240Z",
     "start_time": "2019-11-18T09:19:26.666103Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "treeG=DecisionTreeClassifier(max_depth=treeOpt.best_params_['max_depth'])\n",
    "treeG.fit(Xr_train,Yb_train)\n",
    "plot_tree(treeG,feature_names=dfC.columns.tolist());\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What about the interpretation of the tree? Compare the roles of the variables with the logit model.\n",
    "\n",
    "**Exercise** Complete the results using the [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) function to model concentration; optimize depth, calculate test sample prediction, plot residuals, calculate MSE on test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R's *randomForest* library uses the historic program developed by [Breiman and Cutler](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm) (2001) and interfaced by [Liaw and Wiener](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf). This interface is still being updated, but it's not certain that the original program has continued to evolve since 2004. For large sample sizes, a few thousand, this implementation reaches prohibitive execution times (cf. this [example](https://github.com/wikistat/Ateliers-Big-Data/blob/master/2-MNIST/Atelier-MNIST-R.ipynb)), unlike the Python implementation whose memory management and parallelization capacity have been finely optimized by [Louppe et al.](http://fr.slideshare.net/glouppe/accelerating-random-forests-in-scikitlearn) (2014). \n",
    "\n",
    "Like boosting, two forest functions are offered in `scikit-learn`; one for regression and one for classification, as well as a \"more random\" version. Compared with the original version of R, there are fewer options, but the basic use is very similar, with the same set of parameters.\n",
    "\n",
    "**Question** Identify the parameters and default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# definition of the parameters\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "   criterion='gini', max_depth=None,\n",
    "   min_samples_split=2, min_samples_leaf=1, \n",
    "   max_features='auto', max_leaf_nodes=None,\n",
    "   bootstrap=True, oob_score=True)\n",
    "# Learning\n",
    "rfFit = forest.fit(Xr_train,Yb_train)\n",
    "print(1-rfFit.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the out-of-bag error with the one obtained on the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error on the  test sample\n",
    "1-rfFit.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation optimization of the number of variables randomly drawn during the construction of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=[{\"max_features\":list(range(2,10,1))}]\n",
    "rf= GridSearchCV(RandomForestClassifier(n_estimators=100),\n",
    "        param,cv=5,n_jobs=-1)\n",
    "rfOpt=rf.fit(Xr_train, Yb_train)\n",
    "# paramètre optimal\n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - rfOpt.best_score_,rfOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several runs, randomized by cross-validation, can lead to different \"optimal\" values for this parameter without compromising the quality of the prediction on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error on the  test sample\n",
    "1-rfOpt.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "y_chap = rfFit.predict(Xr_test)\n",
    "# Confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Test different values of min_samples_split from the optimal one. Conclusion on the sensitivity of optimizing this parameter?\n",
    "\n",
    "\n",
    "As with R, it is possible to calculate a variable importance indicator to help with some form of interpretation. This depends on the position of the variable in the tree and therefore corresponds to R's mean decrease in Gini index rather than the mean descrease in accuracy. The forest must be re-estimated, as GridSearch does not know the importance parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators=100,max_features=2)\n",
    "rfFit=rf.fit(Xr_train, Yb_train)\n",
    "# Decreasing importance of variables\n",
    "importances = rfFit.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(Xr_train.shape[1]):\n",
    "    print(dfC.columns[indices[f]], importances[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of importances\n",
    "plt.figure()\n",
    "plt.title(\"Importance des variables\")\n",
    "plt.bar(range(Xr_train.shape[1]), importances[indices]);\n",
    "plt.xticks(range(Xr_train.shape[1]), indices);\n",
    "plt.xlim([-1, Xr_train.shape[1]]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Compare the variable importances with the selections made previously.\n",
    "\n",
    "**Exercise** Then replace the `RandomForestClassifier` function with the `RandomForestRegressor` regression function. Optimize the parameter, calculate the forecast, residuals and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 3 : Neural Networks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks (multilayer perceptron) are only included in the `Scikit-learn` package from version 0.18. Deep learning\" methods require the installation of the [*theano*](http://deeplearning.net/software/theano/) and [*Lasagne*](http://lasagne.readthedocs.io/en/latest/index.html) libraries or [*theano*](http://deeplearning.net/software/theano/), [*TensorFlow*](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html) and [*Keras*](https://keras.io/) libraries. The latter are considerably more complex to install, especially under Windows. They will be the subject of another tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:26.835311Z",
     "start_time": "2019-11-18T09:19:26.821031Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of parameters, including the number of neurons and `alpha`, which sets the default regularization of 10-5. The number of neurons is optimized, but it can be `alpha` with a large number of neurons. The default maximum number of iterations (200) seems insufficient. It is set to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:38.412805Z",
     "start_time": "2019-11-18T09:19:26.836996Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid=[{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}]\n",
    "nnet= GridSearchCV(MLPClassifier(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xr_train, Yb_train)\n",
    "# optimal parameter\n",
    "print(\"Best score = %f, Best parameter = %s\" % (1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:38.422719Z",
     "start_time": "2019-11-18T09:19:38.414606Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Prediction error on the  test sampl\n",
    "1-nnetOpt.score(Xr_test,Yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:38.505699Z",
     "start_time": "2019-11-18T09:19:38.424800Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Prediction on the  test sample\n",
    "y_chap = nnetOpt.predict(Xr_test)\n",
    "# confusion matrix\n",
    "table=pd.crosstab(y_chap,Yb_test)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Replace the MLPClassifier function with the [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) regression function. Optimize the parameter, calculate the forecast, residuals and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis : comparison of the algorithms  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any method, the prediction of whether or not a test will be exceeded is associated with the choice of a threshold, which by default is 0.5. Optimizing this threshold depends on the respective costs associated with false positives and false negatives, which are not necessarily equal. The ROC curve shows the influence of this threshold on the false positive and true positive rates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:19:44.198684Z",
     "start_time": "2019-11-18T09:19:44.193534Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "listMethod=[[\"RF\",rfOpt],[\"NN\",nnetOpt],[\"Tree\",treeOpt],[\"K-nn\",knnOpt],[\"Logit\",logitOpt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:20:02.322746Z",
     "start_time": "2019-11-18T09:19:44.200268Z"
    }
   },
   "outputs": [],
   "source": [
    "for method in enumerate(listMethod):\n",
    "    probas_ = method[1][1].fit(Xr_train, Yb_train).predict_proba(Xr_test)\n",
    "    fpr, tpr, thresholds = roc_curve(Yb_test, probas_[:,1])\n",
    "    plt.plot(fpr, tpr, lw=1,label=\"%s\"%method[1][0])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**  Can the AUC (area under the curve) criterion be used to order the curves and therefore the methods? \n",
    "\n",
    " Rather, it's at an acceptable false-positive rate, and therefore at a fixed threshold value, that we need to choose the preferred learning method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 5 :  Industrialization of learning </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration on several  test samples :*Monte Carlo* cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he test sample is small, so the forecast error estimate may have a large variance. This is reduced by performing a form of cross-validation (*Monte Carlo*), drawing several pairs of training and test samples to iterate the previous treatments. Data are normalized for all methods.\n",
    "\n",
    "Scikit-learn's functionality lends itself well to automating these processes, which involve extracting samples, estimating several models, optimizing their parameters and estimating the prediction error on the test.\n",
    "\n",
    "The code is compact and efficient to run, as it is well parallelized by the functions used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T14:51:22.438772Z",
     "start_time": "2019-07-03T14:50:38.969063Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "import time\n",
    "check_random_state(13)\n",
    "tps0=time.perf_counter()\n",
    "# definition of the estimators\n",
    "logit= LogisticRegression(penalty=\"l1\",solver=\"liblinear\")\n",
    "knn  = KNeighborsClassifier()\n",
    "tree = DecisionTreeClassifier()\n",
    "nnet = MLPClassifier(max_iter=600)\n",
    "rf   = RandomForestClassifier(n_estimators=100)\n",
    "svm  = SVC()\n",
    "# Number of itérations\n",
    "B=3 # Too small value  ! To execute after the  test, it is better to take at least B=30\n",
    "# definition  of the  parameters\n",
    "listMethGrid=[[svm,{\"C\":[0.4,0.5,0.6,0.8,1,1.4]}],\n",
    "    [rf,{\"max_features\":list(range(2,10,2))}],\n",
    "    [nnet,{\"hidden_layer_sizes\":list([(5,),(6,),(7,),(8,)])}],\n",
    "    [tree,{\"max_depth\":list(range(2,10))}],\n",
    "    [knn,{\"n_neighbors\":list(range(1,15))}],\n",
    "    [logit,{\"C\":[0.5,1,5,10,12,15,30]}]]\n",
    "# Initialization at 0 of the errors for each method (column)  and each iteration (line)\n",
    "arrayErreur=np.empty((B,6))\n",
    "for i in range(B):   # iterations on  B test samples\n",
    "    # extraction  of learning and test samples\n",
    "    X_train,X_test,Yb_train,Yb_test=train_test_split(dfC,Yb,test_size=200)\n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train)  \n",
    "    X_train = scaler.transform(X_train)  \n",
    "    # Same transformation for the  test\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # optimisation  of each method  and computation of the test error\n",
    "    for j,(method, grid_list) in enumerate(listMethGrid):\n",
    "        methodGrid=GridSearchCV(method,grid_list,cv=10,n_jobs=-1).fit(X_train, Yb_train)\n",
    "        methodOpt = methodGrid.best_estimator_\n",
    "        methFit=methodOpt.fit(X_train, Yb_train)\n",
    "        arrayErreur[i,j]=1-methFit.score(X_test,Yb_test)\n",
    "tps1=time.perf_counter()\n",
    "print(\"Temps execution en mn :\",(tps1 - tps0))\n",
    "dataframeErreur=pd.DataFrame(arrayErreur,columns=[\"SVM\",\"RF\",\"NN\",\"Tree\",\"Knn\",\"Logit\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T14:51:28.031583Z",
     "start_time": "2019-07-03T14:51:27.827667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution of the prediction errors\n",
    "#.\n",
    "dataframeErreur[[\"SVM\",\"RF\",\"NN\",\"Tree\",\"Knn\",\"Logit\"]].boxplot(return_type='dict')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T14:51:46.326851Z",
     "start_time": "2019-07-03T14:51:46.320143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Means\n",
    "dataframeErreur.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion \n",
    "**Question** Which method(s) would you retain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, processed in R and then in Python, sums up the interest and context of learning methods.\n",
    "* Compared with the *base line*: MOCAGE prediction with an average error rate of 30%, an elementary statistical model significantly improves the result.\n",
    "* A more sophisticated method, in this case *SVM* or *random forest*, provides a statistically significant improvement, but only at the cost of the fine-tuned interpretation of results provided by logistic regression.\n",
    "* Python, a machine learning tool, is more efficient than R for simulations.\n",
    "* On the other hand, R, a \"statistical learning\" tool, enables the selection and interpretation of variables and their **interactions** for a classic linear or logistic regression model. Taking interactions into account (quadratic model) significantly improves forecast quality.\n",
    "* Random forests and SVMs do better on this example, as is often the case with *boosting*, but other examples highlight other methods: neurons for physical modeling, SVMs for virtual screening of molecules, PLS regression for near infrared spectrometry (NIR)... no general rule.\n",
    "* Jupyter is an effective teaching aid for analysis without extensive code development.\n",
    "* Before moving on to [Julia](http://julialang.org/), R and Python are highly complementary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "333.133px",
    "left": "528px",
    "top": "179.283px",
    "width": "231.05px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
