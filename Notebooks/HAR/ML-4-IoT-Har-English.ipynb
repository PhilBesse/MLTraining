{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Machine Learning Tutorial](https://github.com/wikistat/MLTraining): IoT and Human Activity Recognition (HAR)\n",
    "## Analysis of   signals   obtained from a *smartphone* \n",
    "## Use of the libraries <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"Scikit-Learn\"/></a> in <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> and <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a> \n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "The data we will study all along this notebook are public data, which were acquired and described by [Anguita et al. (2013)](). They are available on the [bucket](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) of Irvine California University. They represent usecases of Human Activity Recognition from signal recordings (gyroscope, accelerometer) obtained with a smartphone. \n",
    "The data are analyzed to illustrate the main steps common in *data science* and applicable to sampled physical signals. Visualization of the raw signals to assess the difficulties posed by this type of data; exploration ([principal component analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf), [discriminant factor analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf)) of the transformed (*features*) or *business* data computed from the signals; prediction of the activity from the business data by most linear methods including: [logistic regression](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf) and nonlinear; activity prediction from raw signals by [neural network](http://wikistat.fr/pdf/st-m-app-rn.pdf) of multilayer perceptron type. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Introduction\n",
    "###  General objective\n",
    "The objective is to recognize the activity of an individual carrying a smartphone that records a set of signals from the onboard gyroscope and accelerometer and thus connected. A learning database has been built experimentally. A set of wearers of a smartphone produced a determined activity during a predefined period of time while signals were recorded. The data are from the human activity recognition (HAR) community. See the [article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) reporting on a 2013 conference.  Data analysis associated with real-time activity identification is not addressed.\n",
    "\n",
    "The available public data were acquired, described and partially analyzed by [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf). They are accessible on the [repository](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) at the University California Irvine (UCI) dedicated to machine learning.\n",
    "\n",
    "The archive contains the raw data: accelerations sampled at 64 htz for 2s. The accelerations in x, y, and z, each of 128 columns, those in y subtracting the natural gravity as well as the angular accelerations (gyroscope) in x, y, and z, that is to say 9 files in all. The choice of a power of 2 for the sampling frequency allows the efficient execution of Fourier transform or wavelet algorithms.\n",
    "\n",
    "\n",
    "###  Process\n",
    "A first visualization and exploration of the raw signals shows (section 2) that they are difficult to analyze; the activity classes are indeed poorly characterized. The main cause is the lack of synchronization of the activity beginnings; the phase shift of the signals then appears as a noise or artifact very detrimental for a good discrimination of the activities on the basis of a usual Euclidean distance ($L_2$). This is the reason why [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf) propose to compute a set of classical transformations or features (*features*) of signal processing: variance, correlations, entropy, Fourier decompositions... These are then $p=561$ variables which are considered and explored in section 3. The [principal component analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf) and especially the [discriminant factor analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf) show the good discriminatory qualities of these feature data resulting from an expert knowledge of the signals. Section 4 exploits these features  and shows that elementary statistical models (logistic regression, discriminant analysis) or a classical support vector machine (SVM) algorithm using a simple linear kernel lead to excellent predictions in contrast to sophisticated non-linear algorithms (*random forest*).\n",
    "\n",
    "Nevertheless, having sophisticated transformations computed all the time is not a viable solution for the battery of a connected embedded object. The candidate algorithm must be able to produce a solution that can be integrated (wired) into a circuit, as is the case, for example, with chips dedicated to facial recognition. This is the purpose of section 5: to show the feasibility of a solution based on raw signals only; solution using neural networks.\n",
    "\n",
    "\n",
    "###  Software environment\n",
    "To be executed, this notebook (*jupyter notebook*) requires the installation of Python 3 via for example the site [Anaconda](https://conda.io/docs/user-guide/install/download.html). The exploration and statistical learning algorithms used are available in the [`Scikit-learn`] library (http://scikit-learn.org/stable/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preliminary study of raw signals\n",
    "### Source\n",
    "\n",
    "The data are from the [UCI] repository (https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). It can be downloaded beforehand by clicking [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Each record or statistical unit or instance is labeled with **6 activities**: standing, sitting, lying, walking, climbing or descending stairs. Each dataset is split into a training sample and a test sample. The test sample is only used to evaluate and compare the predictive qualities of the main methods. It is kept as it is in order to make comparisons  with the results of the literature. It is a *supervised classification* problem (6 classes) with $n=10299$ samples for learning, 2947 for testing.\n",
    "\n",
    "The data contains two sets of different dimensions:\n",
    "\n",
    "1. Multidimensional set: an individual consists of 9 Time Series of *dimensions* $(n, 128, 9)$.\n",
    "2. One-dimensional set: The 9 Time Series are concatenated to constitute a vector of 128*9 = 1152 variables of *dimensions* $(n, 1152)$.\n",
    "\n",
    "*N.B.* The data structure is significantly more complex than those commonly studied in the [Wikistat repository](https://github.com/wikistat/). The code has been structured in a sequence of functions in order to facilitate understanding. The *notebook* tool reaches its limits here for the realization of complex codes.\n",
    "\n",
    "\n",
    "###  Importation of the main libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.403179Z",
     "start_time": "2020-04-14T09:09:20.486909Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Structure the data\n",
    "Define the access path to the data and the useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.419530Z",
     "start_time": "2020-04-14T09:09:21.405490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention: the path below must be adapted to the context\n",
    "DATADIR_UCI = './UCI HAR Dataset'\n",
    "# List of file names to automate the reading.\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "# Functions to read the sequence of files before restructuring the data \n",
    "# in the required format.\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = data_dir+'/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
    "    x = my_read_csv(filename).values\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))    \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = data_dir+'/'+subset+'/y_'+subset+'.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).values\n",
    "    else:\n",
    "        Y = y.values\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.563886Z",
     "start_time": "2020-04-14T09:09:21.423726Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of the dimensions to ensure the correct reading of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.579575Z",
     "start_time": "2020-04-14T09:09:31.566388Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "This phase is essential for a good understanding of the data, their structure and therefore of the problems that will be raised later. Visualization is very basic from a methodological point of view but requires more elaborate skills in Python and therefore some preliminary functions. \n",
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.609094Z",
     "start_time": "2020-04-14T09:09:31.584773Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of colors\n",
    "CMAP = plt.get_cmap(\"Accent\")\n",
    "# List of signal types\n",
    "SIGNALS = [\"body_acc x\", \"body_acc y\", \"body_acc z\", \n",
    "                \"body_gyro x\", \"body_gyro y\", \"body_gyro z\", \n",
    "               \"total_acc x\", \"total_acc y\", \"total_acc z\"] \n",
    "# Plain language dictionary of experienced activities (supervised context)\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "labels = ACTIVITY_DIC.values()\n",
    "\n",
    "# Function to draw a signal\n",
    "def plot_one_axe(X, fig, ax, sample_to_plot, cmap):\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            ax.plot(x, linewidth=1, color=cmap(act-1))\n",
    "def plot_one_axe_shuffle(X, fig, ax, sample_to_plot, cmap):\n",
    "    plot_data = []\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            plot_data.append([x,cmap(act-1)])\n",
    "    random.shuffle(plot_data)\n",
    "    for x,color in plot_data:\n",
    "        ax.plot(x, linewidth=1, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Plotting of all signals\n",
    "All signals are plotted by type by overlaying the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:44.902940Z",
     "start_time": "2020-04-14T09:09:31.611691Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 50\n",
    "index_per_act = [list(zip(np.repeat(act, sample_to_plot), np.where(Y_train_label==act)[0][:sample_to_plot])) for act in range(1,7)]\n",
    "index_to_plot = list(itertools.chain.from_iterable(index_per_act))\n",
    "random.shuffle(index_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for isignal in range(9):\n",
    "    ax = fig.add_subplot(3,3,isignal+1)\n",
    "    for act , i in index_to_plot:\n",
    "        ax.plot(range(128), X_train[i,:,isignal],color=CMAP(act-1), linewidth=1)\n",
    "        ax.set_title(SIGNALS[isignal])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Appreciate the difficulty of distinguishing activities within a signal.\n",
    "\n",
    "###  By signal \n",
    "The single signal \"acceleration in x\" is plotted by distinguishing activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.526860Z",
     "start_time": "2020-04-14T09:09:44.904853Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 10\n",
    "isignal = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train_label==act)[0][:sample_to_plot]) for act in range(1,7)])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8), num=SIGNALS[isignal])\n",
    "for act , index in index_per_act_dict.items():\n",
    "    ax = fig.add_subplot(3,2,act)\n",
    "    for x in X_train[index]:\n",
    "        ax.plot(range(128), x[:,0],color=CMAP(act-1), linewidth=1)\n",
    "    ax.set_title(ACTIVITY_DIC[act])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which activity seems to be distinguished easily from the others? \n",
    "\n",
    "**Q.** Observe the signals of an activity, for example `Walking upstairs`. What is it about these signals or curves that makes a classical Euclidean metric ($L_2$) inoperative? \n",
    "\n",
    "This is the reason why it will be important to decompose in particular the signals in the frequency domain. \n",
    "\n",
    "\n",
    "###  [Principal component analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "It is important to get a clear picture of the structure of the data.  A principal component analysis is suitable for this purpose. \n",
    "#### Notes\n",
    "   - PCA is not reduced on these data because this transformation has no effect on the poor quality of the graphs.\n",
    "   - The PCA based on a usual Euclidean metric only confirms the difficulties previously identified and the lack of discriminating power of the raw data in the sense of this metric; this exploration is not deepened on these data. On the other hand, another [notebook](https://github.com/wikistat/Exploration/blob/master/HumanActivityRecognition/Explo-Python-Har-brutes.ipynb) details a discriminant factor analysis but with the same conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below displays a point cloud in a factorial plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.540744Z",
     "start_time": "2020-04-14T09:09:46.530594Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R, ytrain, fig, ax, nbc, nbc2, label_dic=ACTIVITY_DIC, cmaps = plt.get_cmap(\"Accent\")\n",
    "):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = label_dic[i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=10, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=15)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  PCA on a signal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:47.938156Z",
     "start_time": "2020-04-14T09:09:46.548289Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "# Choice of the signal\n",
    "isignal = 4\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"ACP Sur signal : \" +signal)\n",
    "X_c = pca.fit_transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the shares of variance explained by the first principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:48.644737Z",
     "start_time": "2020-04-14T09:09:47.940412Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliquée \\n par les premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the box plots are very disturbed by the distributions of the components with a very high concentration around 0 and a lot of atypical values. Hence the use of the `whis=100` parameter to lengthen the whiskers.\n",
    "\n",
    "**Question** What are the graphs above? What interpretations or lack of interpretation can be drawn from them?\n",
    "\n",
    "**Representation of the first factorial design:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:49.039696Z",
     "start_time": "2020-04-14T09:09:48.646489Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other factorial designs are not much more informative.\n",
    "#### On all signals\n",
    "All signals are concatenated flat into a single signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.091255Z",
     "start_time": "2020-04-14T09:09:49.041222Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "print(\"ACP Sur tous les signaux\")\n",
    "X_c = pca.fit_transform(X_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.852861Z",
     "start_time": "2020-04-14T09:09:52.099072Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliqué \\n des premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:53.240748Z",
     "start_time": "2020-04-14T09:09:52.854459Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Which activity seems easy to identify ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Exploration of the features\n",
    "###  The data\n",
    "The [UCI archive]() also contains two files `train` and `test` of the 561 features, variables computed in the time and frequency domains by transforming the raw signals.\n",
    "\n",
    "Here is an indicative list of the variables computed on each of the raw signals or pairs of signals:\n",
    "\n",
    "Name|Signification\n",
    "-|-\n",
    "mean | Mean value\n",
    "std | Standard deviation\n",
    "mad | Median absolute value\n",
    "max | Largest values in array\n",
    "min | Smallest value in array\n",
    "sma | Signal magnitude area\n",
    "energy | Average sum of the squares\n",
    "iqr | Interquartile range\n",
    "entropy | Signal Entropy\n",
    "arCoeff | Autorregresion coefficients\n",
    "correlation | Correlation coefficient\n",
    "maxFreqInd | Largest frequency component\n",
    "meanFreq | Frequency signal weighted average\n",
    "skewness | Frequency signal Skewness\n",
    "kurtosis | Frequency signal Kurtosis\n",
    "energyBand | Energy of a frequency interval\n",
    "angle | Angle between two vectors\n",
    "\n",
    "#### Reading the features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.976847Z",
     "start_time": "2020-04-14T09:09:53.243035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the training data\n",
    "# Attention, there can be several spaces as separator in the file\n",
    "Xtrain=pd.read_csv(\"X_train.txt\",sep='\\s+',header=None)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.985255Z",
     "start_time": "2020-04-14T09:09:54.979066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target variable \n",
    "ytrain=pd.read_csv(\"y_train.txt\",sep='\\s+',header=None,names=['y'])\n",
    "# The dataFrame type is useless and even embarrassing for the following\n",
    "ytrain=ytrain[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.696175Z",
     "start_time": "2020-04-14T09:09:54.987714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the test data\n",
    "Xtest=pd.read_csv(\"X_test.txt\",sep='\\s+',header=None)\n",
    "Xtest.shape\n",
    "ytest=pd.read_csv(\"y_test.txt\",sep='\\s+',header=None,names=['y'])\n",
    "ytest=ytest[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Principal Component Analysis](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Graphical function for factorial designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.705248Z",
     "start_time": "2020-04-14T09:09:55.699148Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R,fig,ax,nbc,nbc2):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = ACTIVITY_DIC [i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=1, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=10)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the principal components matrix. It is also a change of basis; from the canonical basis to the basis of eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.414735Z",
     "start_time": "2020-04-14T09:09:55.708292Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_c = pca.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues or variance of principal components\n",
    "Representation of the decay of the eigenvalues, the variances of the variables or principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.688255Z",
     "start_time": "2020-04-14T09:09:56.418044Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more explicit graph describes the distributions of these components by box plots; only the former are displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:57.183945Z",
     "start_time": "2020-04-14T09:09:56.691838Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(X_c[:,0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the decrease in variances, the possible choice of a dimension or number of components to retain out of the 561.\n",
    "#### Representation of individuals or \"activities\" in PCA\n",
    "Projection in the main factorial plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.523613Z",
     "start_time": "2020-04-14T09:09:57.186510Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaps = plt.get_cmap(\"Accent\")\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_c, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment on the separation of the two types of situations by the first axis.\n",
    "\n",
    "**Q** What can you say about the shape of the clouds?\n",
    "\n",
    "**Q.** What can you say about the separation of the classes?\n",
    "#### Representation of the variables in PCA\n",
    "Reading the labels of the variables and making a list. Because of the large dimension (561, the representations are not very usable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.542767Z",
     "start_time": "2020-04-14T09:09:59.530303Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('features.txt', 'r') as content_file:\n",
    "    featuresNames = content_file.read()\n",
    "columnsNames = list(map(lambda x : x.split(\" \")[1],featuresNames.split(\"\\n\")[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of variables unreadable by putting the labels in clear. Only a * is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.018314Z",
     "start_time": "2020-04-14T09:09:59.546646Z"
    }
   },
   "outputs": [],
   "source": [
    "# coordinates of the variables\n",
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j in zip(coord1,coord2, ):\n",
    "    plt.text(i, j, \"*\")\n",
    "    plt.arrow(0,0,i,j,color='r')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='b', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identification of the variables participating most in the first axis. It is not clearer! Only the representation of the individuals finally brings elements of understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.026378Z",
     "start_time": "2020-04-14T09:10:03.020595Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.array(columnsNames)[abs(coord1)>.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Factorial Discriminant Analysis (FDA)](http://wikistat.fr/pdf/st-m-explo-afd.pdf)\n",
    "#### Principle\n",
    "PCA does not take into account the presence of the qualitative variable to be modeled, contrary to the Factorial Disciminant Analysis (FDA) adapted to this \"supervised\" context since the activity is known on a learning sample. The FDA is a PCA of the barycenters of the classes providing the space of the individuals with a specific metric called *Mahalanobis*. This metric is defined by the inverse of the intraclass covariance matrix. The objective is then to visualize the capacities of the variables to discriminate the classes.\n",
    "\n",
    "The `scikit-learn` library does not propose a specific function of discriminant factorial analysis but the coordinates of the individuals in the basis of the discriminant vectors are obtained as results of the decisional linear discriminant analysis. This last one will be used with a predictive purpose in a second time (other notebook). \n",
    "\n",
    "The results of the `LinearDiscriminantAnalysis` function of `scikit-learn` are identical to those of the `lda` function of R. It is thus used strictly in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:04.460102Z",
     "start_time": "2020-04-14T09:10:03.032128Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "method = LinearDiscriminantAnalysis() \n",
    "lda=method.fit(Xtrain,ytrain)\n",
    "X_r2=lda.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What does *warning* mean? What processing would be required to use decision discriminant analysis in modeling or learning?\n",
    "\n",
    "#### Representation of individuals in FDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:06.668576Z",
     "start_time": "2020-04-14T09:10:04.464352Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_r2, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What about the separation of classes? Are they all separable two by two ?\n",
    "\n",
    "**Q.** What about the shape of the clouds, especially in the first plane ?\n",
    "\n",
    "As for the PCA, the representation of the variables is too complex and would not be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Unsupervised classification](http://wikistat.fr/pdf/st-m-explo-classif.pdf)\n",
    "This section is not useful since the classes are known. Nevertheless, a general approach to the study of signals relating to unidentified human activities *a priori* would require this unsupervised classification or *clustering* phase. This step simply illustrates the behavior of a classical unsupervised classification algorithm. The confusion matrix of the obtained classes with the known classes allows to evaluate the performance of the algorithm. \n",
    "#### $k$*-means*\n",
    "Beware, it is necessary to center and reduce the variables before running an unsupervised classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.356260Z",
     "start_time": "2020-04-14T09:10:06.670363Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tps1 = time.perf_counter()\n",
    "X = StandardScaler().fit_transform(Xtrain)\n",
    "km=KMeans(n_clusters=6)\n",
    "km.fit(Xtrain)\n",
    "tps2 = time.perf_counter()\n",
    "print(\"Temps execution Kmeans :\", (tps2 - tps1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.426455Z",
     "start_time": "2020-04-14T09:10:11.360623Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(ytrain, km.labels_)[1:7,0:6].T, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What about the effectiveness of an unsupervised approach to categorizing activities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Activity prediction from the features \n",
    "Several methods are successively tested in this notebook: SVM, decision discriminate analysis, $k$ nearest neighbors, random forests, neural networks... We start with logistic regression for behavior prediction.\n",
    "\n",
    "### [Logistic regression](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "\n",
    "#### Principle\n",
    "An old statistical method but finally efficient on these data. Logistic regression is adapted to the prediction of a binary variable. In the multiclass case, the logistic function of the `Scikit-learn` library estimates *by default* **a model by class**: one class against the others. \n",
    "\n",
    "The probability of an individual belonging to a class is modeled using a linear combination of the explanatory variables. To transform a linear combination with value in $\\mathbb{R}$ into a probability with value in the interval $[0, 1]$, a sigmoidal function is applied.  This gives: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ or, equivalently, a linear decomposition of the *logit* or *log odd ratio* of $P(y_i=1)$: $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$\n",
    "\n",
    "\n",
    "#### Estimation of the model without optimization\n",
    "The model is estimated without trying to refine the values of some parameters (penalization). This will be done in a second step. The parameter of choice of the *solver* is specified because the default choice (`lbfgs`) seems to converge less quickly. A systematic comparison of the different options (`liblinear, lbfgs, saga, sag, newton-cg`) would be welcome in association with the choice of the model when the number of classes is greater than 2 : multinomial loss function or a binomial model per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.398790Z",
     "start_time": "2020-04-14T09:10:11.430699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "method = LogisticRegression(solver='liblinear',multi_class='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the activity of the test sample\n",
    "Once the model has been estimated, the prediction error is evaluated, without optimistic bias, on another sample, the so-called test sample, which did not participate in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.424156Z",
     "start_time": "2020-04-14T09:10:18.400699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which classes are still difficult to discriminate?\n",
    "\n",
    "**Q.** Comment on the quality of the results obtained. Are they consistent with the exploratory approach?\n",
    "\n",
    "#### Optimization of the model by Lasso penalization\n",
    "*Warning* the execution is a bit long... this optimization can be omitted in the first reading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.967342Z",
     "start_time": "2020-04-14T09:10:18.428545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimisation of the penalisation parameter\n",
    "# grid of values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[0.5,1,5,10,12,15,30]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto'), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(Xtrain, ytrain)  \n",
    "# optimal parameter\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.973912Z",
     "start_time": "2020-04-14T09:19:19.969854Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.995153Z",
     "start_time": "2020-04-14T09:19:19.976014Z"
    }
   },
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(Xtest)\n",
    "# confusion matrix\n",
    "logitOpt.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:20.021501Z",
     "start_time": "2020-04-14T09:19:19.996900Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Is the improvement significant with respect to the calculation time?\n",
    "\n",
    "**Q.** Determine the variables selected by the LASSO method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Linear Discriminant Analysis](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "\n",
    "**Q.** What about the optimization of this method ? It is proposed in an R library but not available in Python.\n",
    "\n",
    "**Q.** The quadratic discriminant analysis poses some problems. Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.325921Z",
     "start_time": "2020-04-14T09:19:20.024699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "ts = time.time()\n",
    "method = LinearDiscriminantAnalysis()\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.356007Z",
     "start_time": "2020-04-14T09:19:21.327902Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [*K* nearest neighbors](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "\n",
    "This method can be seen as a special case of discriminant analysis with local estimation of conditional density functions. \n",
    "\n",
    "**Q.** How many neighbors are used by default? Optimize this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.487153Z",
     "start_time": "2020-04-14T09:19:21.357706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "ts = time.time()\n",
    "method = KNeighborsClassifier(n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.508495Z",
     "start_time": "2020-04-14T09:19:39.488760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Linear SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "\n",
    "The maximum number of iterations has been significantly increased (1000 by default) without improving the performance. The multi-class case is treated by considering one class against the others, thus 6 models.\n",
    "\n",
    "**Q.** Use the default parameter and then vary the penalty parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.714818Z",
     "start_time": "2020-04-14T09:19:39.511096Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "ts = time.time()\n",
    "method = LinearSVC(max_iter=20000)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.740091Z",
     "start_time": "2020-04-14T09:20:08.716703Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [SVM with Gaussian kernel](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "Learning with the default values and then optimize the parameters.\n",
    "\n",
    "**Q.** What are the parameters to be optimized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.973697Z",
     "start_time": "2020-04-14T09:20:08.742383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "ts = time.time()\n",
    "method = SVC(gamma='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.992066Z",
     "start_time": "2020-04-14T09:20:36.975386Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Which procedure is executed below and for what purpose?\n",
    "\n",
    "*Please note*: the execution is a bit long and can be skipped in the first reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.746208Z",
     "start_time": "2020-04-14T09:20:36.993400Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[4,5,6],\"gamma\":[.01,.02,.03]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(Xtrain, ytrain)\n",
    "te = time.time()\n",
    "te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.761547Z",
     "start_time": "2020-04-14T09:24:44.756680Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimal parameter\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Compare the two SVM approaches (linear and radial): computation time and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [*Random forest*](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "\n",
    "**Q.** What would be the parameter to optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.273566Z",
     "start_time": "2020-04-14T09:24:44.764426Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ts = time.time()\n",
    "method = RandomForestClassifier(n_estimators=200,n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.291331Z",
     "start_time": "2020-04-14T09:24:58.275291Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Combining models\n",
    "The shapes of the clouds of each class observed in the first plane of the principal component analysis show that the covariance structure is not identical in each class. This remark would suggest that the quadratic discriminant analysis should be used, but it blocks on the estimation of the six covariance matrices and their inverse. Nevertheless, it seems that, more precisely, two groups can be distinguished: the active classes (walking, going up or down stairs) on the one hand and the passive classes (lying down, sitting down, standing up) on the other hand, and that, within each group, the variances are quite similar. \n",
    "\n",
    "This situation suggests the construction of a two-stage or hierarchical decision:\n",
    "1. Logistic regression separating passive *vs.* active activities,\n",
    "2. A model specific to each of the previous classes, e.g. Gaussian kernel SVMs.\n",
    "\n",
    "Such a hierarchical model construction leads to an accuracy of more than 97%.\n",
    "\n",
    "**Exercise:** Program such an approach using the capabilities of Python to make a *pipeline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Activity prediction from raw signals\n",
    "###  Introduction\n",
    "As explained in the introduction, the computation of numerous data transformations is far too consuming of the battery resources of a connected object. This section proposes to use only raw signals to learn an algorithm and among the possible algorithms, only neural networks that can be \"wired\" into a circuit are considered. We use here a multilayer perceptron. \n",
    "\n",
    "\n",
    "###  One hidden layer perceptron\n",
    "#### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.058629Z",
     "start_time": "2020-04-14T09:28:52.862017Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as km \n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.068108Z",
     "start_time": "2020-04-14T09:28:55.062816Z"
    }
   },
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "def my_confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the network \n",
    "A hidden layer, a reformatting layer and an output layer with 6 classes. The number of neurons (50) on the hidden layer has been optimized elsewhere. The number of epochs and the size of the batches should be optimized especially in the case of using a GPU card.\n",
    "\n",
    "Note the number of parameters to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.273143Z",
     "start_time": "2020-04-14T09:28:55.069755Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "n_hidden = 50\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "model_base_mlp =km.Sequential()\n",
    "model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:30:14.297438Z",
     "start_time": "2020-04-14T09:28:55.275469Z"
    }
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"\\nScore With Simple MLP on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_prediction = model_base_mlp.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Does adding a hidden layer improve the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more efforts would undoubtedly allow to gain a few points on the accuracy of the result but... beware of overfitting if the same test sample is always used. \n",
    "\n",
    "A convolutional network (CNN) on the signals would allow to obtain better performances, close to those obtained on the features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
